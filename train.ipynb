{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5ceaed-870a-4999-a568-9470e132244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-15 15:07:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-05-15 15:07:18 (7.69 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the dataset for training \n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc80853-78de-49e1-8e59-bd989dfaaaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and inspeect the text file\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a486cd48-a98f-4f67-a713-7611718313cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      " \n",
      "\n",
      "length of the dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "# exploring the txt file\n",
    "\n",
    "# looking at first 1000 characters\n",
    "print(text[:1000], '\\n')\n",
    "\n",
    "#number of characters in the dataset\n",
    "print(f\"length of the dataset: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91377efa-3874-421c-8949-d2905c958353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "size of vocabulary: 65\n"
     ]
    }
   ],
   "source": [
    "# vocabulary and the size of vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocabulary: {''.join(chars)}\")\n",
    "print(f\"size of vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab7c85d-f38d-437f-b4f5-6446ae5e9084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the text; ie convert the raw strings to some sequences of integers\n",
    "\n",
    "# creating a mapping of characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda st: [stoi[x] for x in st] # function of the string which takes the string and returns a list of integers\n",
    "decode = lambda l: ''.join([itos[x] for x in l]) # function of the lost of integers where it takes each elements of the list and returns a character and joins them together\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3bca78-a635-478d-8e1d-941420d5e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# encoding the entire dataset and storing it as a torch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d859c8-aaf5-4259-b1d2-65484daaae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4bb3a0-7f9c-4086-b0d8-cd4861ca855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and validation sets\n",
    "n = int(0.9 * len(data)) # first 90% will be train, rest 10% will be validation set\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# validation set will help us understand how much our model is overfitting and memorising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e12f8a-3f86-406e-962c-b0861d193a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the entire text is never fed into the transformer in one go as it is computationally prohibitive\n",
    "# chunking the dataset and sample these chunks into the transformer, thus training transformer on chunks at a time\n",
    "block_size = 8 # chunk size\n",
    "\"\"\"seeing first 9 chars\n",
    "we are chunking for 8 chars. Since each preceding sequence of characters must predict for the next character. so, 18 would predict 47, 18 and 47 would predict 56.\n",
    "So this way we have actually 8 individual training examples here\n",
    "\"\"\"\n",
    "train_data[:block_size + 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d277a1-b55d-4a17-a51a-47e16022bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example 1: When input: tensor([18]), target: 47\n",
      " Example 2: When input: tensor([18, 47]), target: 56\n",
      " Example 3: When input: tensor([18, 47, 56]), target: 57\n",
      " Example 4: When input: tensor([18, 47, 56, 57]), target: 58\n",
      " Example 5: When input: tensor([18, 47, 56, 57, 58]), target: 1\n",
      " Example 6: When input: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      " Example 7: When input: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      " Example 8: When input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[: block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #context is all preceding chars plus current character\n",
    "    target = y[t] \n",
    "    print(f\" Example {t+1}: When input: {context}, target: {target}\")\n",
    "\n",
    "# this masked type attention is done to make transformer used to seeing all kinds of lengths of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf244ff0-0420-41f4-a048-f174341e4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8]) shape, \n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets: torch.Size([4, 8]) shape, \n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------------\n",
      "when input is [24], the target is 43\n",
      "when input is [24, 43], the target is 58\n",
      "when input is [24, 43, 58], the target is 5\n",
      "when input is [24, 43, 58, 5], the target is 57\n",
      "when input is [24, 43, 58, 5, 57], the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1], the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
      "when input is [44], the target is 53\n",
      "when input is [44, 53], the target is 56\n",
      "when input is [44, 53, 56], the target is 1\n",
      "when input is [44, 53, 56, 1], the target is 58\n",
      "when input is [44, 53, 56, 1, 58], the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46], the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
      "when input is [52], the target is 58\n",
      "when input is [52, 58], the target is 1\n",
      "when input is [52, 58, 1], the target is 58\n",
      "when input is [52, 58, 1, 58], the target is 46\n",
      "when input is [52, 58, 1, 58, 46], the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39], the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
      "when input is [25], the target is 17\n",
      "when input is [25, 17], the target is 27\n",
      "when input is [25, 17, 27], the target is 10\n",
      "when input is [25, 17, 27, 10], the target is 0\n",
      "when input is [25, 17, 27, 10, 0], the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21], the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n"
     ]
    }
   ],
   "source": [
    "# we stack the batches of chunks into stacks and then feed them into the transformer. \n",
    "# we will have many batches of many chunks of text that are all stacked up in a single tensor.\n",
    "# we do this because GPUs are very good at paralle processing.\n",
    "# These chunks are trained upon independently\n",
    "\n",
    "# we will start sampling random locations in the dataset to pull chunks from \n",
    "\n",
    "torch.manual_seed(1337) # we are setting the seed for the random number generator so that the random locations pulled are same whenever we train the model\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) # we generate batch_size number of random offsets, ie 4 values. \n",
    "    # these 4 values should be between 0 and len(data)-block_size\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # torch.stack to take all 4 of the 1-D tensors and stack them up as rows in a 4x8 tensor\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'inputs: {xb.shape} shape, \\n{xb}')\n",
    "print(f'\\ntargets: {yb.shape} shape, \\n{yb}')\n",
    "print('----------------')\n",
    "for b in range(batch_size):\n",
    "    for t in range (block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'when input is {context.tolist()}, the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95cd707d-9a09-4fd9-957e-14d22dabdd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# feeding the input into very simple bigram neural network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\"\"\"\n",
    "bigram model:\n",
    "If you input 'h', it tries to predict 'e', 'i', etc., based only on 'h'.\n",
    "It doesn't look at what came before 'h' (no 't', 's', etc. before it — just 'h').\n",
    "\n",
    "# -------------------------------\n",
    "# Bigram Model Explanation\n",
    "# -------------------------------\n",
    "# We're using an nn.Embedding(vocab_size, vocab_size) layer here, which may look like a typical embedding layer,\n",
    "# but it's not used in the traditional sense.\n",
    "#\n",
    "# In most NLP models, embeddings represent tokens (e.g., characters or words) as low-dimensional dense vectors\n",
    "# that capture semantic meaning and are used as input to deeper layers.\n",
    "#\n",
    "# However, in this bigram model, we use the embedding table to directly map each input token (an integer index)\n",
    "# to a vector of size vocab_size that represents the raw logits (unnormalized scores) for the next character.\n",
    "# So:\n",
    "#     Input token index → Lookup corresponding row in embedding table → Row used as logits for next character\n",
    "#\n",
    "# We use nn.Embedding here simply because it's a convenient way to index a learnable weight matrix using integers.\n",
    "# The output of this embedding is not a dense semantic vector — it's the actual logits for the next prediction step.\n",
    "\n",
    "# Dimensions used:\n",
    "# B = Batch size       → Number of sequences processed in parallel\n",
    "# T = Time steps       → Number of tokens in each sequence\n",
    "# C = Channels         → Size of output vector per token, here equal to vocab_size (i.e., number of possible next tokens)\n",
    "\n",
    "# At this stage, the model has not been trained — so the logits it outputs are essentially random.\n",
    "# Training (via loss and backpropagation) will adjust the embedding table so that each input token learns to\n",
    "# predict the most likely next token based on character-level bigram statistics.\n",
    "\n",
    "\"\"\"\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) Batch: 4, time: 8, channels is vocab size: 65\n",
    "        # when we pass idx here, every integer in our input is going to refer to this embedding table \n",
    "        # and is going to pluck out a row of that embedding table corresponding to its index\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:            \n",
    "            # pytorch is then going to arrange all of this into a batch X time X channel tensor\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # we are doing this because cross entropy loss function in pytorch expects a B,C,T tensor\n",
    "            targets = targets.view(B*T) # right now the targets are of shape B,T, we are making it 1D.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss # logits are the scores for the next character in the seq\n",
    "        # we are predicting the next char just based on just individual identity of a single token.\n",
    "        # currenly the tokens are not seeing any context or interacting with each other \n",
    "        # this can still do some predictions\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # gives us a running stream of generations.\n",
    "        # idx is the (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus on only the last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C) for the T+1 char, basically we are plucking out thr last element in the time dimension.\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1) in each of the batch dimensions, we will have a single prediction for what comes next \n",
    "            # append sampled index to the running sequence \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            # whatver is predicted is concatenated on top of previous idx along the time dimension. so basically this whole thing takes (B,T)\n",
    "            # and makes it to (B,T+1), (B, T+2) and so on upto max_new_tokens.\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits,loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx_trial = torch.zeros((1,1), dtype=torch.long )# creating a tensor where batch is 1 and time is 1, and its value is 0. the datatype is integer\n",
    "print(decode(m.generate(idx_trial, max_new_tokens = 100)[0].tolist())) # because generate works on the level of batches, we have to get the 0th row\n",
    "# to get the single batch dimension that is getting output, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07ad411-ac1f-40c7-9400-415151fa3dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're using:\\nvocab_size = 65 → 65 possible characters\\nA randomly initialized model, so:\\nThe logits (output of the embedding table) are random, and therefore\\nAfter softmax, the probability distribution over the next character is nearly uniform (i.e., ~1/65 for each character).\\nCross-Entropy Loss:\\nCross-entropy compares the predicted probability distribution (p̂) with the true distribution (p = one-hot).\\n\\nIf the model predicts all 65 characters equally likely (uniform distribution), and the true next character is 'e', then:\\n\\nloss = − log(predicted\\xa0prob\\xa0of\\xa0correct\\xa0token) = − log(1/65) ≈ 4.17\\nSo yes -ln(1/65) is the expected loss for a uniform random model.\\n\\nso, what does loss tell us?\\nf you get a loss close to 4.17 before training:\\nIt’s a good sanity check — your model is producing an almost uniform distribution, as expected.\\nIf the loss is much lower, something might be wrong (like model memorizing already, or bad labels).\\nIf the loss is higher, your softmax might be too “spiky” (concentrated on a few wrong tokens), which can happen if your logits are initialized poorly.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can guess what the loss should be\n",
    "# we are expecting -ln(1/65) ~ 4.17\n",
    "\"\"\"You're using:\n",
    "vocab_size = 65 → 65 possible characters\n",
    "A randomly initialized model, so:\n",
    "The logits (output of the embedding table) are random, and therefore\n",
    "After softmax, the probability distribution over the next character is nearly uniform (i.e., ~1/65 for each character).\n",
    "Cross-Entropy Loss:\n",
    "Cross-entropy compares the predicted probability distribution (p̂) with the true distribution (p = one-hot).\n",
    "\n",
    "If the model predicts all 65 characters equally likely (uniform distribution), and the true next character is 'e', then:\n",
    "\n",
    "loss = − log(predicted prob of correct token) = − log(1/65) ≈ 4.17\n",
    "So yes -ln(1/65) is the expected loss for a uniform random model.\n",
    "\n",
    "so, what does loss tell us?\n",
    "f you get a loss close to 4.17 before training:\n",
    "It’s a good sanity check — your model is producing an almost uniform distribution, as expected.\n",
    "If the loss is much lower, something might be wrong (like model memorizing already, or bad labels).\n",
    "If the loss is higher, your softmax might be too “spiky” (concentrated on a few wrong tokens), which can happen if your logits are initialized poorly.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cba17ad-c5cb-4a34-8cb0-4065b7907d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the bigram model\n",
    "# creating a pytorch optimizer object \n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebbc1e57-dbbe-489b-89e1-0efc99d4188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4522743225097656\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# training for 100 epochs\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # zeroing out all gradients from previous step\n",
    "    loss.backward() # getting the gradients for all of the params\n",
    "    optimizer.step() # using the gradients to updte the params\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723aa4e6-96b8-4bd8-9c1e-73f69d8adb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DUKIVisun Casshe wisthiot s.\n",
      "LUK:\n",
      "\n",
      "NGOLI io e alllker s?j$NCowens l het hislaspicobar, heay ind, cigigeluandac! thaforo nont\n",
      "SLO:\n",
      "Ange ive nn I ou m,\n",
      "UCENTheanp'Lbet bazzl\n",
      "TEEXNore t b'Thathon:\n",
      "sous min'd ne st wousis s lingilo whee,\n",
      "K:\n",
      "Toow'e's,\n",
      "D:\n",
      "NGLEng, do te! ase may sin ceecate.\n",
      "God? d\n",
      "Aw ht h\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long ), max_new_tokens = 300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77120b-569d-4e19-85d4-bb3b064511d9",
   "metadata": {},
   "source": [
    "## mathematical trick for self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69116350-5a8d-4601-ad3a-c7bf237f3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2\n",
    "x = torch.rand(B,T,C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c724871d-f39f-4c3a-9606-8f1d6ad6626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have upto 8 tokens in batch which are currently not interacting with each other\n",
    "# to couple them in such a way that a token only interacts with its preceding tokens, because we want to predict the future tokens\n",
    "# the easiest way for tokens to interact is taking average of all the preceding tokens and current token.\n",
    "# it becomes the feature vector that summarises the current vector in contxt of its history\n",
    "# this commuication is extremely lossy as it has lost all positional information about tokens.\n",
    "\n",
    "# we want x[b, t] = mean_{i<t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(b):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C) # t: how many preceding tokens\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # averaging out over the 0th dimension ie averaging out T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00148627-d024-4e01-b7f0-78e800b0e3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0783, 0.4956],\n",
       "        [0.6231, 0.4224],\n",
       "        [0.2004, 0.0287],\n",
       "        [0.5851, 0.6967],\n",
       "        [0.1761, 0.2595],\n",
       "        [0.7086, 0.5809],\n",
       "        [0.0574, 0.7669],\n",
       "        [0.8778, 0.2434]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73074a05-6fe3-4198-922c-ababf9baf0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0783, 0.4956],\n",
       "        [0.3507, 0.4590],\n",
       "        [0.3006, 0.3156],\n",
       "        [0.3717, 0.4108],\n",
       "        [0.3326, 0.3806],\n",
       "        [0.3953, 0.4140],\n",
       "        [0.3470, 0.4644],\n",
       "        [0.4134, 0.4368]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ebc101-e167-4d57-9fcb-b9e395168918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that in x[0] and xbow[0], ,the first row is the same since its the same token\n",
    "#(bow is bag of words which usually represents a simple average of token representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deca839-6573-41b5-8a7a-d9be988c37ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b97f1c67-65bd-4587-bb1c-b5dece5d2899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--------\n",
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c = tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "\n",
      "Triangular matrix a: tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "a = tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--------\n",
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c = tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this way of calculating average is very inefficient, instead we can use matrices\n",
    "# matrix multiplication\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b= torch.randint(0,10, (3,2)).float() # a matrix of size 3,2 and it should have random values between 0 and 10\n",
    "c = a @ b\n",
    "print(f\"a = {a}\\n--------\\nb = {b}\\n----------\\nc = {c}\\n\")\n",
    "\n",
    "# using matrix multiplication to get average over tokens\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "print(f'Triangular matrix a: {a}\\n')\n",
    "\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = b= torch.randint(0,10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(f\"a = {a}\\n--------\\nb = {b}\\n----------\\nc = {c}\\n\")\n",
    "# we can see in c that first row is average of just first row of b, second row is avrage of first and second row of b and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a675b8b3-cd8e-4f2f-aa3f-f5df9ee57bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(1, keepdim = True)\n",
    "xbow2 = wei@x # (B, T, T) @ (B, T, C)---> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31b7cb98-59bf-4d3d-9d95-b832752bfa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0783, 0.4956],\n",
       "         [0.3507, 0.4590],\n",
       "         [0.3006, 0.3156],\n",
       "         [0.3717, 0.4108],\n",
       "         [0.3326, 0.3806],\n",
       "         [0.3953, 0.4140],\n",
       "         [0.3470, 0.4644],\n",
       "         [0.4134, 0.4368]]),\n",
       " tensor([[0.0783, 0.4956],\n",
       "         [0.3507, 0.4590],\n",
       "         [0.3006, 0.3156],\n",
       "         [0.3717, 0.4108],\n",
       "         [0.3326, 0.3806],\n",
       "         [0.3953, 0.4140],\n",
       "         [0.3470, 0.4644],\n",
       "         [0.4134, 0.4368]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0] # they are the same \n",
    "# we are leveraging batch matrix multiplication to basically get a weighted average of all tokens in a batch\n",
    "#all the tokens post current tooken are weighted down to 0 using a triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be60c701-17e2-4485-aa00-7e848fdda30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei matrix after masked fill: tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "wei after applying softmax: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to get a masked matrix of weights to get average of preceding tokens\n",
    "# method using softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # wherever tril is 0, we make that -inf in the wei\n",
    "print(f'wei matrix after masked fill: {wei}\\n')\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(f'wei after applying softmax: {wei}\\n') \n",
    "xbow3 = wei@x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae4be634-1af2-48b2-b3e1-b82796c74fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masked self attention without averaging but using Q,K,V\n",
    "# ie getting wei which is dependent on data\n",
    "\"\"\"\n",
    "how this works:\n",
    "every single node/token will emit two vectors: query and key\n",
    "query: what am i looking for\n",
    "key: what do i contain\n",
    "the way we get affinities between tokens now is we do a dot product between keys and tokens\n",
    "\"\"\"\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# lets see a single head perform self attention \n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x)\n",
    "# when i forward these linears on top of x, all of the positions in the b,t arrangement in parallel and independently produce a key and a query\n",
    "# the communication happens now\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# now we do masking\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09fa7599-5c5a-40b2-a023-4efbe81d686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0] # wei now tells us how much information to aggregate from all of the tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dc2c594-d2d8-4abf-bf96-7af60d922918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnotes about self attention:\\n- there is no notion of position, attention simply acts over a set of vectors. thats why we need to positionally encode the input tokens.\\n- each chunk over batch dimension is processed completely independently and never talk to each other.\\n- it basically means matrix multiplication is aplied in parallel across the batch dim\\n- if our batch size is 4, that means that we have separate pools of 8 nodes and those nodes only talk to each other.\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "notes about self attention:\n",
    "- there is no notion of position, attention simply acts over a set of vectors. thats why we need to positionally encode the input tokens.\n",
    "- each chunk over batch dimension is processed completely independently and never talk to each other.\n",
    "- it basically means matrix multiplication is aplied in parallel across the batch dim\n",
    "- if our batch size is 4, that means that we have separate pools of 8 nodes and those nodes only talk to each other.\n",
    "\n",
    "difference between self and cross attention\n",
    "self-attention: keys, queries and values are all coming from the same source, from x\n",
    "cross-attention: queries are coming from x but the keys and the values come from separate source.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f7cc8bc-0234-4f63-a7db-50cf4b412882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance without scaling: (tensor(1.0172), tensor(0.9599), tensor(12.9863))\n",
      "variance with scaling: (tensor(1.0172), tensor(0.9599), tensor(0.8116))\n",
      "softmax when tensor values are diffused: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "softmax when tensor values are spiky and go very low or very high: tensor([3.2932e-04, 8.1630e-07, 1.7980e-02, 8.1630e-07, 9.8169e-01])\n"
     ]
    }
   ],
   "source": [
    "# scaled attention normalisation method\n",
    "# we have gaussian distributed values with unit variance and zero mean for k and q\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(f\"variance without scaling: {k.var(), q.var(), wei.var()}\")\n",
    "\n",
    "#variance for wei (if calculated naively) is of the order of head_size\n",
    "\n",
    "# but if we multiply by head_size**-0.5\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print(f\"variance with scaling: {k.var(), q.var(), wei.var()}\")\n",
    "\n",
    "# we need to do this because wei gets fed into the softmax function, so it needs to be fairly diffused. \n",
    "# otherwise, if wei takes on very posisitive or very negative numbers inside it, softmax will actially converge towards one hot vectors\n",
    "# this would mean eery node would end up aggregating information from only one single other node\n",
    "# example\n",
    "print(f\"softmax when tensor values are diffused: {torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)}\")\n",
    "print(f\"softmax when tensor values are spiky and go very low or very high: {torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*20, dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562be47-8ede-4974-8eb4-66e80a7c4461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
