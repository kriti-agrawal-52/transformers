{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5ceaed-870a-4999-a568-9470e132244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-15 15:07:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-05-15 15:07:18 (7.69 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the dataset for training \n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc80853-78de-49e1-8e59-bd989dfaaaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and inspeect the text file\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a486cd48-a98f-4f67-a713-7611718313cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      " \n",
      "\n",
      "length of the dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "# exploring the txt file\n",
    "\n",
    "# looking at first 1000 characters\n",
    "print(text[:1000], '\\n')\n",
    "\n",
    "#number of characters in the dataset\n",
    "print(f\"length of the dataset: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91377efa-3874-421c-8949-d2905c958353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "size of vocabulary: 65\n"
     ]
    }
   ],
   "source": [
    "# vocabulary and the size of vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocabulary: {''.join(chars)}\")\n",
    "print(f\"size of vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab7c85d-f38d-437f-b4f5-6446ae5e9084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the text; ie convert the raw strings to some sequences of integers\n",
    "\n",
    "# creating a mapping of characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda st: [stoi[x] for x in st] # function of the string which takes the string and returns a list of integers\n",
    "decode = lambda l: ''.join([itos[x] for x in l]) # function of the lost of integers where it takes each elements of the list and returns a character and joins them together\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3bca78-a635-478d-8e1d-941420d5e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# encoding the entire dataset and storing it as a torch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d859c8-aaf5-4259-b1d2-65484daaae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4bb3a0-7f9c-4086-b0d8-cd4861ca855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and validation sets\n",
    "n = int(0.9 * len(data)) # first 90% will be train, rest 10% will be validation set\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# validation set will help us understand how much our model is overfitting and memorising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e12f8a-3f86-406e-962c-b0861d193a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the entire text is never fed into the transformer in one go as it is computationally prohibitive\n",
    "# chunking the dataset and sample these chunks into the transformer, thus training transformer on chunks at a time\n",
    "block_size = 8 # chunk size\n",
    "\"\"\"seeing first 9 chars\n",
    "we are chunking for 8 chars. Since each preceding sequence of characters must predict for the next character. so, 18 would predict 47, 18 and 47 would predict 56.\n",
    "So this way we have actually 8 individual training examples here\n",
    "\"\"\"\n",
    "train_data[:block_size + 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d277a1-b55d-4a17-a51a-47e16022bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example 1: When input: tensor([18]), target: 47\n",
      " Example 2: When input: tensor([18, 47]), target: 56\n",
      " Example 3: When input: tensor([18, 47, 56]), target: 57\n",
      " Example 4: When input: tensor([18, 47, 56, 57]), target: 58\n",
      " Example 5: When input: tensor([18, 47, 56, 57, 58]), target: 1\n",
      " Example 6: When input: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      " Example 7: When input: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      " Example 8: When input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[: block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #context is all preceding chars plus current character\n",
    "    target = y[t] \n",
    "    print(f\" Example {t+1}: When input: {context}, target: {target}\")\n",
    "\n",
    "# this masked type attention is done to make transformer used to seeing all kinds of lengths of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf244ff0-0420-41f4-a048-f174341e4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8]) shape, \n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets: torch.Size([4, 8]) shape, \n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------------\n",
      "when input is [24], the target is 43\n",
      "when input is [24, 43], the target is 58\n",
      "when input is [24, 43, 58], the target is 5\n",
      "when input is [24, 43, 58, 5], the target is 57\n",
      "when input is [24, 43, 58, 5, 57], the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1], the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
      "when input is [44], the target is 53\n",
      "when input is [44, 53], the target is 56\n",
      "when input is [44, 53, 56], the target is 1\n",
      "when input is [44, 53, 56, 1], the target is 58\n",
      "when input is [44, 53, 56, 1, 58], the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46], the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
      "when input is [52], the target is 58\n",
      "when input is [52, 58], the target is 1\n",
      "when input is [52, 58, 1], the target is 58\n",
      "when input is [52, 58, 1, 58], the target is 46\n",
      "when input is [52, 58, 1, 58, 46], the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39], the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
      "when input is [25], the target is 17\n",
      "when input is [25, 17], the target is 27\n",
      "when input is [25, 17, 27], the target is 10\n",
      "when input is [25, 17, 27, 10], the target is 0\n",
      "when input is [25, 17, 27, 10, 0], the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21], the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n"
     ]
    }
   ],
   "source": [
    "# we stack the batches of chunks into stacks and then feed them into the transformer. \n",
    "# we will have many batches of many chunks of text that are all stacked up in a single tensor.\n",
    "# we do this because GPUs are very good at paralle processing.\n",
    "# These chunks are trained upon independently\n",
    "\n",
    "# we will start sampling random locations in the dataset to pull chunks from \n",
    "\n",
    "torch.manual_seed(1337) # we are setting the seed for the random number generator so that the random locations pulled are same whenever we train the model\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) # we generate batch_size number of random offsets, ie 4 values. \n",
    "    # these 4 values should be between 0 and len(data)-block_size\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # torch.stack to take all 4 of the 1-D tensors and stack them up as rows in a 4x8 tensor\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'inputs: {xb.shape} shape, \\n{xb}')\n",
    "print(f'\\ntargets: {yb.shape} shape, \\n{yb}')\n",
    "print('----------------')\n",
    "for b in range(batch_size):\n",
    "    for t in range (block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'when input is {context.tolist()}, the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95cd707d-9a09-4fd9-957e-14d22dabdd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# feeding the input into very simple bigram neural network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\"\"\"\n",
    "bigram model:\n",
    "If you input 'h', it tries to predict 'e', 'i', etc., based only on 'h'.\n",
    "It doesn't look at what came before 'h' (no 't', 's', etc. before it — just 'h').\n",
    "\n",
    "# -------------------------------\n",
    "# Bigram Model Explanation\n",
    "# -------------------------------\n",
    "# We're using an nn.Embedding(vocab_size, vocab_size) layer here, which may look like a typical embedding layer,\n",
    "# but it's not used in the traditional sense.\n",
    "#\n",
    "# In most NLP models, embeddings represent tokens (e.g., characters or words) as low-dimensional dense vectors\n",
    "# that capture semantic meaning and are used as input to deeper layers.\n",
    "#\n",
    "# However, in this bigram model, we use the embedding table to directly map each input token (an integer index)\n",
    "# to a vector of size vocab_size that represents the raw logits (unnormalized scores) for the next character.\n",
    "# So:\n",
    "#     Input token index → Lookup corresponding row in embedding table → Row used as logits for next character\n",
    "#\n",
    "# We use nn.Embedding here simply because it's a convenient way to index a learnable weight matrix using integers.\n",
    "# The output of this embedding is not a dense semantic vector — it's the actual logits for the next prediction step.\n",
    "\n",
    "# Dimensions used:\n",
    "# B = Batch size       → Number of sequences processed in parallel\n",
    "# T = Time steps       → Number of tokens in each sequence\n",
    "# C = Channels         → Size of output vector per token, here equal to vocab_size (i.e., number of possible next tokens)\n",
    "\n",
    "# At this stage, the model has not been trained — so the logits it outputs are essentially random.\n",
    "# Training (via loss and backpropagation) will adjust the embedding table so that each input token learns to\n",
    "# predict the most likely next token based on character-level bigram statistics.\n",
    "\n",
    "\"\"\"\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) Batch: 4, time: 8, channels is vocab size: 65\n",
    "        # when we pass idx here, every integer in our input is going to refer to this embedding table \n",
    "        # and is going to pluck out a row of that embedding table corresponding to its index\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:            \n",
    "            # pytorch is then going to arrange all of this into a batch X time X channel tensor\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # we are doing this because cross entropy loss function in pytorch expects a B,C,T tensor\n",
    "            targets = targets.view(B*T) # right now the targets are of shape B,T, we are making it 1D.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss # logits are the scores for the next character in the seq\n",
    "        # we are predicting the next char just based on just individual identity of a single token.\n",
    "        # currenly the tokens are not seeing any context or interacting with each other \n",
    "        # this can still do some predictions\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): # gives us a running stream of generations.\n",
    "        # idx is the (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus on only the last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C) for the T+1 char, basically we are plucking out thr last element in the time dimension.\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1) in each of the batch dimensions, we will have a single prediction for what comes next \n",
    "            # append sampled index to the running sequence \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            # whatver is predicted is concatenated on top of previous idx along the time dimension. so basically this whole thing takes (B,T)\n",
    "            # and makes it to (B,T+1), (B, T+2) and so on upto max_new_tokens.\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits,loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx_trial = torch.zeros((1,1), dtype=torch.long )# creating a tensor where batch is 1 and time is 1, and its value is 0. the datatype is integer\n",
    "print(decode(m.generate(idx_trial, max_new_tokens = 100)[0].tolist())) # because generate works on the level of batches, we have to get the 0th row\n",
    "# to get the single batch dimension that is getting output, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07ad411-ac1f-40c7-9400-415151fa3dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're using:\\nvocab_size = 65 → 65 possible characters\\nA randomly initialized model, so:\\nThe logits (output of the embedding table) are random, and therefore\\nAfter softmax, the probability distribution over the next character is nearly uniform (i.e., ~1/65 for each character).\\nCross-Entropy Loss:\\nCross-entropy compares the predicted probability distribution (p̂) with the true distribution (p = one-hot).\\n\\nIf the model predicts all 65 characters equally likely (uniform distribution), and the true next character is 'e', then:\\n\\nloss = − log(predicted\\xa0prob\\xa0of\\xa0correct\\xa0token) = − log(1/65) ≈ 4.17\\nSo yes -ln(1/65) is the expected loss for a uniform random model.\\n\\nso, what does loss tell us?\\nf you get a loss close to 4.17 before training:\\nIt’s a good sanity check — your model is producing an almost uniform distribution, as expected.\\nIf the loss is much lower, something might be wrong (like model memorizing already, or bad labels).\\nIf the loss is higher, your softmax might be too “spiky” (concentrated on a few wrong tokens), which can happen if your logits are initialized poorly.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can guess what the loss should be\n",
    "# we are expecting -ln(1/65) ~ 4.17\n",
    "\"\"\"You're using:\n",
    "vocab_size = 65 → 65 possible characters\n",
    "A randomly initialized model, so:\n",
    "The logits (output of the embedding table) are random, and therefore\n",
    "After softmax, the probability distribution over the next character is nearly uniform (i.e., ~1/65 for each character).\n",
    "Cross-Entropy Loss:\n",
    "Cross-entropy compares the predicted probability distribution (p̂) with the true distribution (p = one-hot).\n",
    "\n",
    "If the model predicts all 65 characters equally likely (uniform distribution), and the true next character is 'e', then:\n",
    "\n",
    "loss = − log(predicted prob of correct token) = − log(1/65) ≈ 4.17\n",
    "So yes -ln(1/65) is the expected loss for a uniform random model.\n",
    "\n",
    "so, what does loss tell us?\n",
    "f you get a loss close to 4.17 before training:\n",
    "It’s a good sanity check — your model is producing an almost uniform distribution, as expected.\n",
    "If the loss is much lower, something might be wrong (like model memorizing already, or bad labels).\n",
    "If the loss is higher, your softmax might be too “spiky” (concentrated on a few wrong tokens), which can happen if your logits are initialized poorly.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cba17ad-c5cb-4a34-8cb0-4065b7907d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the bigram model\n",
    "# creating a pytorch optimizer object \n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebbc1e57-dbbe-489b-89e1-0efc99d4188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4522743225097656\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# training for 100 epochs\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # zeroing out all gradients from previous step\n",
    "    loss.backward() # getting the gradients for all of the params\n",
    "    optimizer.step() # using the gradients to updte the params\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723aa4e6-96b8-4bd8-9c1e-73f69d8adb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DUKIVisun Casshe wisthiot s.\n",
      "LUK:\n",
      "\n",
      "NGOLI io e alllker s?j$NCowens l het hislaspicobar, heay ind, cigigeluandac! thaforo nont\n",
      "SLO:\n",
      "Ange ive nn I ou m,\n",
      "UCENTheanp'Lbet bazzl\n",
      "TEEXNore t b'Thathon:\n",
      "sous min'd ne st wousis s lingilo whee,\n",
      "K:\n",
      "Toow'e's,\n",
      "D:\n",
      "NGLEng, do te! ase may sin ceecate.\n",
      "God? d\n",
      "Aw ht h\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long ), max_new_tokens = 300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457cb4d-601e-4b43-b2a7-5283a9d0f846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
